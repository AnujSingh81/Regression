{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1-What is Simple Linear Regression?\n",
        "\n",
        "->simple linear regression is a method to model the relationship between one independent variable(x) and one dependent variable(y) using a strainght line. it predict Y based on linear regression equation The goal is to minimize prediction errors."
      ],
      "metadata": {
        "id": "e1FWjC1108ht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "->The Key assumption of simple linear regressionare:Linearity,independence, Homoscedasitictiy,Normalility, No multicollinearity"
      ],
      "metadata": {
        "id": "Nb58YVVL2ITA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3-What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "->the coefficent of m in the equation represents the slop of the line"
      ],
      "metadata": {
        "id": "WshbjGD92tRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4- What does the intercept c represent in the equation Y=mX+c?\n",
        "\n",
        "->in the equation, the intercept(c) represent the value of Y when X=0."
      ],
      "metadata": {
        "id": "teIMZmaV3BwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5-How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "->In Simple Linear Regression, the slope (m) is calculale\n",
        "\n",
        "It measures how much the dependent variable (Y) changes for each one-unit increase in the independent variable (X)."
      ],
      "metadata": {
        "id": "PGbpKsuB3YPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6-What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "->The least squares method in Simple Linear Regression is used to find the best-fitting line through the data. It works by minimizing the sum of the squared differences between the actual values and the predicted values, ensuring the line gives the most accurate predictions overall."
      ],
      "metadata": {
        "id": "8VPpCUGy32jj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7-How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "\n",
        "->In Simple Linear Regression, the coefficient of determination (R²) measures how well the regression line fits the data. It shows the proportion of variance in the dependent variable (Y) explained by the independent variable (X).\n",
        "\n",
        "R² = 1 → perfect fit\n",
        "\n",
        "R² = 0 → no relationship"
      ],
      "metadata": {
        "id": "xnYIYomv4I5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8-What is Multiple Linear Regression?\n",
        "\n",
        "->ChatGPT said:\n",
        "\n",
        "Multiple Linear Regression is a statistical method used to model the relationship between one dependent variable (Y) and two or more independent variables (X₁, X₂, X₃, …)."
      ],
      "metadata": {
        "id": "K12vaOLw4ewc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9-What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "->The main difference is in the number of independent variables used:\n",
        "\n",
        "Simple Linear Regression uses one independent variable to predict the dependent variable.\n",
        "\n",
        "Multiple Linear Regression uses two or more independent variables to predict the dependent variable, allowing for more complex and accurate modeling."
      ],
      "metadata": {
        "id": "OuJxYARm4rt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10-What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "->The key assumptions of Multiple Linear Regression are:\n",
        "\n",
        "Linearity -The relationship between dependent and independent variables is linear.\n",
        "\n",
        "Independence -Observations are independent of each other.\n",
        "\n",
        "Homoscedasticity -Constant variance of errors.\n",
        "\n",
        "Normality - Errors are normally distributed.\n",
        "\n",
        "No Multicollinearity - Independent variables are not highly correlated."
      ],
      "metadata": {
        "id": "BBDIod_F44ii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11-What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "->Heteroscedasticity occurs when the variance of the error terms in a regression model is not constant across all levels of the independent variables.\n",
        "\n",
        "It affects the model by making the standard errors unreliable, which can lead to incorrect t-tests, p-values, and confidence intervals, reducing the model's accuracy and reliabilit"
      ],
      "metadata": {
        "id": "xYQoomaJ5Muq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12-How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "->To improve a Multiple Linear Regression model with high multicollinearity, you can:\n",
        "\n",
        "1.Remove highly correlated variables.\n",
        "2.Combine correlated predictors using techniques like Principal Component Analysis (PCA).\n",
        "\n",
        "3.Use regularization methods such as Ridge or Lasso regression.\n",
        "\n",
        "4.Increase sample size to reduce variance."
      ],
      "metadata": {
        "id": "Pr4V5jL95dyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13-What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "->Common techniques for transforming categorical variables for regression models include:\n",
        "\n",
        "\n",
        "1.Label Encoding-Assigns numeric labels to categories.\n",
        "\n",
        "2.One-Hot Encoding-Creates binary columns (0/1) for each category.\n",
        "\n",
        "3.Ordinal Encoding-Used when categories have a meaningful order.\n",
        "\n",
        "4.Target Encoding-Replaces categories with the mean of the target variable.\n",
        "\n"
      ],
      "metadata": {
        "id": "CSRGtOj558HZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14-What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "->Interaction terms in Multiple Linear Regression capture the combined effect of two or more independent variables on the dependent variable.\n",
        "\n",
        "They show how the relationship between one predictor and the outcome changes depending on the level of another predictor, helping the model represent non-additive or joint effects between variables."
      ],
      "metadata": {
        "id": "-nmrEBkO6bMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15-How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "->In Simple Linear Regression, the intercept represents the value of Y when X = 0.\n",
        "\n",
        "In Multiple Linear Regression, the intercept represents the predicted value of Y when all independent variables (X₁, X₂, …) are zero.\n",
        "However, this may not always be meaningful if zero isn’t realistic for all predictors."
      ],
      "metadata": {
        "id": "GyAk-Zz-68kO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16-What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "->The slope in regression analysis shows how much the dependent variable (Y) changes when an independent variable (X) increases by one unit, keeping other variables constant.\n",
        "\n",
        "It indicates the direction and strength of the relationship —\n",
        "\n",
        "Positive slope: Y increases with X\n",
        "\n",
        "Negative slope: Y decreases with X"
      ],
      "metadata": {
        "id": "mzwaKiae7Y_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17-- How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "->The intercept in a regression model provides the starting point of the relationship between variables. It represents the predicted value of the dependent variable (Y) when all independent variables (X's) are zero.\n",
        "\n",
        "It helps give context to the regression line by anchoring where the prediction begins, though sometimes it may lack practical meaning if zero isn't realistic for all predictors."
      ],
      "metadata": {
        "id": "nagIGVIp7rj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18-What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        "->The limitations of using R² as the only measure of model performance are:\n",
        "\n",
        "1.Doesn't indicate causation — a high R² doesn't mean X causes Y.\n",
        "\n",
        "2.Increases with more variables, even irrelevant ones.\n",
        "\n",
        "3.Doesn't measure prediction accuracy on new data.\n",
        "\n",
        "4.Ignores model bias and overfitting."
      ],
      "metadata": {
        "id": "a3oasbuNLXVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19-How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "->A large standard error for a regression coefficient means the estimate of that coefficient is unstable or uncertain.\n",
        "\n",
        "It suggests that the variable's effect on the dependent variable is not precisely measured, possibly due to multicollinearity, small sample size, or high data variability, reducing confidence in that predictor's significance."
      ],
      "metadata": {
        "id": "V26Y9jd8L3X5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20-How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "->Heteroscedasticity can be identified in residual plots when the residuals show a pattern or changing spread—for example, forming a cone or funnel shape instead of being randomly scattered.\n",
        "\n",
        "It's important to address because it violates regression assumptions, leading to biased standard errors, unreliable hypothesis tests, and inaccurate confidence intervals or predictions."
      ],
      "metadata": {
        "id": "Csby2z7_MTgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21-What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "\n",
        "->If a Multiple Linear Regression model has a high R² but low adjusted R², it means the model includes irrelevant or non-significant variables.\n",
        "\n",
        "While R² always increases with more predictors, adjusted R² penalizes unnecessary ones.\n",
        "Thus, a big gap suggests overfitting or too many variables that don't actually improve the model's performance."
      ],
      "metadata": {
        "id": "6HrVav0OMlwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22-Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "->It's important to scale variables in Multiple Linear Regression because:\n",
        "\n",
        "1.It ensures all variables contribute equally to the model.\n",
        "\n",
        "2.It prevents large-scale variables from dominating the regression coefficients.\n",
        "\n",
        "3.It improves the numerical stability of calculations.\n",
        "\n",
        "4.It's essential when using regularization methods like Ridge or Lasso regression."
      ],
      "metadata": {
        "id": "fOEFKHrANEDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23-What is polynomial regression?\n",
        "\n",
        "->Polynomial Regression is a type of regression analysis where the relationship between the independent variable (X) and the dependent variable (Y) is modeled as an nth-degree polynomial."
      ],
      "metadata": {
        "id": "olqPIQV-NnGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24- How does polynomial regression differ from linear regression?\n",
        "\n",
        "->Polynomial Regression differs from Linear Regression in the form of the relationship it models:\n",
        "\n",
        "Linear Regression fits a straight line — assumes a linear relationship between X and Y.\n",
        "\n",
        "Polynomial Regression fits a curved line by adding higher-order terms (like X², X³), allowing it to capture non-linear patterns in the data"
      ],
      "metadata": {
        "id": "0kiosc6UNyDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25-When is polynomial regression used?\n",
        "\n",
        "->Polynomial Regression is used when the relationship between the independent and dependent variables is non-linear but can still be modeled smoothly.\n",
        "\n",
        "It's useful when data shows a curved trend, such as growth rates, price-demand curves, or temperature effects — where a straight line (linear regression) cannot accurately fit the pattern."
      ],
      "metadata": {
        "id": "Gp7SnFYgN9yJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26-What is the general equation for polynomial regression?\n",
        "\n",
        "-> General Polynomial Regression Equation\n",
        "\n",
        "Y = β0 + β1*X + β2*(X**2) + β3*(X**3) + ... + βn*(X**n) + ε\n"
      ],
      "metadata": {
        "id": "VdPVuAv6OMjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27-Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "->Yes, Polynomial Regression can be applied to multiple variables — this is called Multivariate Polynomial Regression.\n",
        "\n",
        "It models relationships that are non-linear in predictors but linear in coefficients."
      ],
      "metadata": {
        "id": "gwf7MwGTOmGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28-What are the limitations of polynomial regression?\n",
        "\n",
        "->The main limitations of Polynomial Regression are:\n",
        "\n",
        "1.Overfitting - High-degree polynomials can fit noise instead of the true pattern.\n",
        "\n",
        "2.Extrapolation risk - Predictions beyond the data range are unreliable.\n",
        "\n",
        "3.Complexity - Difficult to interpret for higher degrees.\n",
        "\n",
        "4.Multicollinearity - Polynomial terms can be highly correlated.\n",
        "\n",
        "5.Computational cost - Increases with degree and number of variables."
      ],
      "metadata": {
        "id": "qhb5dtmfOzR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29-What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "->To evaluate model fit and choose the right polynomial degree, you can use:\n",
        "\n",
        "1.Train-test split - Compare performance on training vs. test data.\n",
        "\n",
        "2.Cross-validation - Check model stability across folds.\n",
        "\n",
        "3.R² and Adjusted R² - Measure goodness of fit while penalizing extra terms.\n",
        "\n",
        "4.AIC/BIC - Penalize overly complex models.\n",
        "\n",
        "5.Residual plots -Inspect patterns to detect underfitting or overfitting."
      ],
      "metadata": {
        "id": "HoYLD90mPMFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30-Why is visualization important in polynomial regression?\n",
        "\n",
        "->Visualization is important in Polynomial Regression because it helps:\n",
        "\n",
        "\n",
        "1.Understand the data pattern - Shows whether a linear or curved relationship exists.\n",
        "\n",
        "\n",
        "2.Choose the right degree - Reveals underfitting or overfitting visually.\n",
        "\n",
        "\n",
        "3.Validate model fit - Compare predicted vs. actual trends.\n",
        "\n",
        "\n",
        "4.Communicate results - Makes complex relationships easier to interpret and explain.\n",
        "\n"
      ],
      "metadata": {
        "id": "JlvvLeJHPtx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31-How is polynomial regression implemented in Python?\n",
        "\n",
        "-># Import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Step 1: Create sample data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([3, 6, 14, 28, 45])\n",
        "\n",
        "# Step 2: Transform features to polynomial (degree 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Step 3: Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Step 4: Predict\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Step 5: Visualize\n",
        "plt.scatter(X, y, color='blue', label='Actual data')\n",
        "plt.plot(X, y_pred, color='red', label='Polynomial fit')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AYA-hdwEQDIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BGNaxdiS7U11"
      }
    }
  ]
}